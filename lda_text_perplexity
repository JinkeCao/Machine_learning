from __future__ import print_function
# -*- coding: utf-8 -*-
"""
Created on Tue May  8 14:18:09 2018

@author: jcao2014
"""

from time import time, sleep
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation
from jieba import cut
from pandas import read_table
from json import loads
from sklearn.model_selection import GridSearchCV
from matplotlib import pyplot
def text_cleaner(x):
    raw_text = ''
    if x != 'null':
        x = loads(x)
        for i in x:
            raw_text = raw_text + i['text'] + ' '
    return ' '.join(cut(raw_text))
print("Loading dataset...")

t0 = time()
df1 = read_table('sample2_text.txt', header=None, names=['dvc', 'phone_num', 'rec_result', 'time_t', 'type', 'source'])
df1 = df1[df1['type']>1]
df1 = df1.rec_result.apply(text_cleaner)
# df1.to_csv('sample1_text_clean.csv')

print(df1.shape, "done in %0.3fs." % (time() - t0))


n_samples = df1.shape[0]
n_features = 1000
n_components = 10
n_top_words = 20


def print_top_words(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        message = "Topic #%d: " % topic_idx
        message += " ".join([feature_names[i]
                             for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)
    print()


# Use tf (raw term count) features for LDA.
print("Extracting tf features for LDA...")
# tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
#                                 max_features=n_features,
#                                 stop_words='english')
tf_vectorizer = CountVectorizer()
t0 = time()
# tf = tf_vectorizer.fit_transform(data_samples)
tf = tf_vectorizer.fit_transform(df1)
# joblib.dump(tf_vectorizer,'sample1_text_tf_Model')

print("done in %0.3fs." % (time() - t0))
# print("Fitting LDA models with tf features, "
#       "n_samples=%d and n_features=%d..."
#       % (n_samples, n_features))
# lda = LatentDirichletAllocation(n_components=300, max_iter=20,
#                                 learning_method='online',
#                                 learning_offset=50.,
#                                 verbose = 1;
#                                 random_state=0)
# t0 = time()
# lda.fit(tf)
# print("done in %0.3fs." % (time() - t0), lda.perplexity(tf))
# sleep(100)
n_topics = range(25, 300, 25)
perplexityLst = [1.0]*len(n_topics)

lda_models = []
for idx, n_topic in enumerate(n_topics):
    lda = LatentDirichletAllocation(n_components=n_topic,
                                    max_iter=20,
                                    learning_method='batch',
                                    evaluate_every=200,
                                    earning_offset=50.,
                                    perp_tol=0.1,                                     
                                    doc_topic_prior=1/n_topic,
                                    topic_word_prior=1/n_topic, 
                                    verbose=0)
    t0 = time()
    lda.fit(tf)
    perplexityLst[idx] = lda.perplexity(tf)
    lda_models.append(lda)
    print("# of Topic: %d" % n_topics[idx], 
#           "done in %0.3fs N_iter %d" % ((time() - t0), lda.n_iter_),
          "Perplexity Score %0.3f" % perplexityLst[idx])

best_index = perplexityLst.index(min(perplexityLst))
best_n_topic = n_topics[best_index]
best_model = lda_models[best_index]
print("Best # of Topic: ", best_n_topic)

fig = pyplot.figure()
ax = fig.add_subplot(1,1,1)
ax.plot(n_topics, perplexityLst)
ax.set_xlabel("# of topics")
ax.set_ylabel("Approximate Perplexity")
pyplot.grid(True)
pyplot.show()


# print("\nTopics in LDA model:")



# # Use tf-idf features for NMF.
# print("Extracting tf-idf features for NMF...")
# # tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,
# #                                    max_features=n_features,
# #                                    stop_words='english')
# tfidf_vectorizer = TfidfVectorizer()
# t0 = time()
# # tfidf = tfidf_vectorizer.fit_transform(data_samples)
# tfidf = tfidf_vectorizer.fit_transform(df1)
# print("done in %0.3fs." % (time() - t0))

# # Fit the NMF model
# print("Fitting the NMF model (Frobenius norm) with tf-idf features, "
#       "n_samples=%d and n_features=%d..."
#       % (n_samples, n_features))
# t0 = time()
# nmf = NMF(n_components=n_components, random_state=1,
#           alpha=.1, l1_ratio=.5).fit(tfidf)
# print("done in %0.3fs." % (time() - t0))

# print("\nTopics in NMF model (Frobenius norm):")
# tfidf_feature_names = tfidf_vectorizer.get_feature_names()
# print_top_words(nmf, tfidf_feature_names, n_top_words)

# # Fit the NMF model
# print("Fitting the NMF model (generalized Kullback-Leibler divergence) with "
#       "tf-idf features, n_samples=%d and n_features=%d..."
#       % (n_samples, n_features))
# t0 = time()
# nmf = NMF(n_components=n_components, random_state=1,
#           beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,
#           l1_ratio=.5).fit(tfidf)
# print("done in %0.3fs." % (time() - t0))

# print("\nTopics in NMF model (generalized Kullback-Leibler divergence):")
# tfidf_feature_names = tfidf_vectorizer.get_feature_names()
# print_top_words(nmf, tfidf_feature_names, n_top_words)


